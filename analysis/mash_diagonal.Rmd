---
title: "Add a diagonal covaraince to mash"
author: "DongyueXie"
date: "2020-09-14"
output:
  workflowr::wflow_html:
    code_folding: hide
editor_options:
  chunk_output_type: console
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = F,message = F)
```


```{r}
library(mashr)
library(mvtnorm)

n_conditions = function(data){ncol(data$Bhat)}

n_effects = function(data){nrow(data$Bhat)}

bovy_wrapper = function(data, Ulist_init, subset=NULL, ...){
  if(is.null(subset)){subset = 1:n_effects(data)}
  K = length(Ulist_init)
  R = n_conditions(data)
  pi_init = rep(1/K, K) # initial mix proportions
  D = ncol(data$V)
  if(all(data$V==diag(D))){
    ed.res = extreme_deconvolution(data$Bhat[subset,],
                                   data$Shat[subset,]^2,
                                   xamp = pi_init,
                                   xmean = matrix(0,nrow=K,ncol=R),
                                   xcovar = Ulist_init,
                                   fixmean = TRUE,
                                   ...)
  }else{
    if(!is.null(data$L)){
      ycovar = lapply(subset, function(i) data$L %*% (data$Shat_orig[i,] * t(data$V * data$Shat_orig[i,])) %*% t(data$L) )
    }else{
      ycovar = lapply(subset, function(i) data$Shat[i,] * t(data$V * data$Shat[i,]) )
    }
    ed.res = extreme_deconvolution(data$Bhat[subset,],
                                   ycovar,
                                   xamp = pi_init,
                                   xmean = matrix(0,nrow=K,ncol=R),
                                   xcovar = Ulist_init,
                                   fixmean = TRUE,
                                   ...)
  }
  return(list(pi = ed.res$xamp, Ulist = ed.res$xcovar, av_loglik = ed.res$avgloglikedata))
}

calc_loglikx = function(data,subset,pihat,Ulist,sigma){
  
  n = length(subset)
  loglik = 0
  for(i in subset){
    loglik = loglik + mixture_loglikx(data$Bhat[i,],data$Shat[i,],pihat,Ulist,sigma)
  }
  loglik
}

mixture_loglikx = function(x,shat,pihat,Ulist,sigma){
  
  K = length(pihat)
  p = length(x)
  lik = 0
  for(k in 1:K){
    #browser()
    lik = lik + pihat[k]*dmvnorm(x,sigma = Ulist[[k]]+diag(shat^2)+sigma^2*diag(p))
  }
  log(lik)
}



fdp = function(dis.idx, true.idx){
  if(length(dis.idx)==0){
    0
  }else{
    1-mean(dis.idx%in%true.idx)
  }
}


auc = function(pred,true.label){
  auc=pROC::roc(response = true.label, predictor = pred,direction = '<',levels = c(0,1))
  auc$auc
}

powr = function(dis.idx, true.idx){
  if(length(dis.idx)==0){
    0
  }else{
    sum(dis.idx%in%true.idx)/length(true.idx)
  }
}

mse = function(x,y){
  mean((x-y)^2)
}

summary_out = function(B,out = list(m.c=m.c,m.ed=m.ed,m.c.ed=m.c.ed,m.true=m.true),
                       alpha=0.05,criteria = 'lfsr'){
  
  # identify genes
  non_null_idx = which(rowSums(B)!=0)
  non_null_idx_c = which(B!=0)
  
  which_null = 1*(rowSums(B)==0)
  which_null_c = 1*(B==0)
  
  fdps = c()
  #aucs = c()
  powers = c()
  
  fdps_c = c()
  #aucs_c = c()
  powers_c = c()
  
  mses = c()
  log_liks = c()
  
  for(i in 1:length(out)){
    
    if(criteria=='lfsr'){
    fdps[i] = fdp(get_significant_results(out[[i]],thresh = alpha),non_null_idx)
    fdps_c[i] = fdp(which(out[[i]]$result$lfsr<alpha),non_null_idx_c)
    
    #aucs[i] = auc(c(apply(out[[i]]$result$lfsr,1,min)),which_null)
    #aucs_c[i] = auc(c(out[[i]]$result$lfsr),c(which_null_c))
    
    powers[i] = powr(get_significant_results(out[[i]],thresh = alpha),non_null_idx)
    powers_c[i] = powr(which(out[[i]]$result$lfsr<alpha),non_null_idx_c)
    
    }
    
    
    if(criteria=='lfdr'){
    fdps[i] = fdp(get_significant_results(out[[i]],thresh = alpha),non_null_idx)
    fdps_c[i] = fdp(which(out[[i]]$result$lfdr<alpha),non_null_idx_c)
    
    #aucs[i] = auc(c(apply(out[[i]]$result$lfdr,1,min)),which_null)
    #aucs_c[i] = auc(c(out[[i]]$result$lfdr),c(which_null_c))
    
    powers[i] = powr(get_significant_results(out[[i]],thresh = alpha),non_null_idx)
    powers_c[i] = powr(which(out[[i]]$result$lfdr<alpha),non_null_idx_c)
    
    }
    
    
    mses[i] = mse(B,out[[i]]$result$PosteriorMean)
    log_liks[i] = get_loglik(out[[i]])
  }
  
  find_genes = rbind(fdps,powers)
  rownames(find_genes) = c('fdp','power')
  colnames(find_genes) = names(out)
  
  find_cond = rbind(fdps_c,powers_c,mses,log_liks)
  rownames(find_cond) = c('fdp','power','mse','log_lik')
  colnames(find_cond) = names(out)
  
  return(list(find_genes=find_genes,find_cond=find_cond,mses=mses))
  
}

simu_study = function(simdata,subset){
  data = mash_set_data(simdata$Bhat,simdata$Shat)
  #m.1by1 = mash_1by1(data)
  #strong = get_significant_results(m.1by1)
  strong = subset
  U.c    = cov_canonical(data)
  U.pca  = cov_pca(data,5,strong)
  U.ed   = cov_ed(data,U.pca,strong)
  
  U.true = simdata$U.true
  m.c    = mash(data, U.c,verbose = F)
  m.ed   = mash(data, U.ed,verbose = F)
  m.c.ed = mash(data, c(U.c,U.ed),verbose = F)
  #m.c.ed.sparse = mash(data, c(U.c,U.ed.sparse),verbose = F)
  m.true = mash(data, U.true,verbose = F)
  out = list(m.c=m.c,m.ed=m.ed,m.c.ed=m.c.ed,m.true=m.true)
  out
}
```


## The problem

Since mash includes data-driven covariance matrices in prior, the uncertainty in estimation results in "non-robustness" of subsequent estimates of lfsr. We start with an example illustrating the problem.   

We generate $X$ from a mixture of 2 multivariate normal distributions, $x_i\sim \sum_k\pi_k N(0,U_k+S_i)$, with the following covariance matrices: present (and identical) in first two conditions, present (and identical) in last three conditions, each with 500 samples(so $\pi_1=\pi_2=0.5$), and $S_i=diag(0.1,...,0.1)$.

Two covariance matrices are:

```{r}
cov1 = c(1,1,0,0,0)%*%t(c(1,1,0,0,0))
cov2 = c(0,0,1,1,1)%*%t(c(0,0,1,1,1))
print(cov1)
print(cov2)
```

```{r}
simple_sims0 = function(nsamp = 100, err_sd = 0.01){
    ncond = 5
    b1 = rnorm(nsamp)
    B.1 = matrix(cbind(b1, b1, 0, 0, 0), nrow = nsamp, ncol = ncond)
    b2 = rnorm(nsamp)
    B.2 = matrix(cbind(0, 0, b2, b2, b2), nrow = nsamp, ncol = ncond)
    
    #B.id = matrix(rnorm(nsamp * ncond), nrow = nsamp, ncol = ncond)
    #B.zero = matrix(0, nrow = nsamp, ncol = ncond)
    
    B = rbind(B.1, B.2)
    Shat = matrix(err_sd, nrow = nrow(B), ncol = ncol(B))
    E = matrix(rnorm(length(Shat), mean = 0, sd = Shat), nrow = nrow(B), ncol = ncol(B))
    Bhat = B + E
    row_ids = paste0("effect_", 1:nrow(B))
    col_ids = paste0("condition_", 1:ncol(B))
    rownames(B) = row_ids
    colnames(B) = col_ids
    rownames(Bhat) = row_ids
    colnames(Bhat) = col_ids
    rownames(Shat) = row_ids
    colnames(Shat) = col_ids
    U = matrix(0,nrow=ncond,ncol=ncond)
    U2 = U
    U2[1:2,1:2] = 1
    U3 = U
    U3[3:5,3:5] = 1
    U.true = list(#U1 = matrix(0,nrow=ncond,ncol=ncond),
                  U2=U2,
                  U3=U3)
                  #U4 = diag(ncond))
    return(list(B = B, Bhat = Bhat, Shat = Shat,U.true=U.true))
}

set.seed(12345)
simdata = simple_sims0(500,err_sd = 0.1)

data = mash_set_data(simdata$Bhat,simdata$Shat)
#m.1by1 = mash_1by1(data)
#strong = get_significant_results(m.1by1)
strong = 1:nrow(simdata$B)
U.c    = cov_canonical(data)
U.pca  = cov_pca(data,5,strong)
ed.out = bovy_wrapper(data,U.pca,strong)
```


We first look at ED estimates of $\pi_k, U_k$'s.

Estimated mixing proportions: 

```{r}
round(ed.out$pi,2)
```

Estimated covariance matrices:

```{r}
lapply(ed.out$Ulist,round,digits=3)
```

Then run mash with estimated prior covariance matrices. We compare 4 different settings: only use canonical ones(m.c), only use ED estimated ones(m.ed), use both canonical ones and ED estimated ones(m.c.ed), and true ones(m.true). 
```{r}
result = simu_study(simdata,1:nrow(simdata$B))
out = summary_out(simdata$B,result)
knitr::kable(out$find_cond,caption = 'On finding conditions',digits = 5)
```

Adding ED estimated covariance matrices inflates FDP while increase power and likelihood, reduces mse. Now look at lfsr.

```{r}
hist(c(result$m.c.ed$result$lfsr),breaks = 100,xlab='lfsr',main="Histogram of lfsr")
hist(result$m.c.ed$result$lfsr[which(simdata$B==0)],breaks = 100,xlab='lfsr',main="Histogram of lfsr for those beta=0")
plot(c(simdata$B), c(result$m.c.ed$result$lfsr),col=1,pch=".",xlab='beta',ylab='lfsr')
abline(h = 0.05,lty=2,col='grey80')
abline(h = 0.1,lty=2,col='grey80')
```


## Add diagonal cov and penalize likelihood


We first obtain estimates of $\pi_k,U_k$ from ED(without diagonal cov) and fix them, then add $\sigma^2I$, calculate log likelihood, and choose $\sigma^2$ such that log-likelihood drops by a factor of 2. The log likelihood is $$\log L(X) = \sum_i\log(\sum_k\pi_k N(x_i;0,U_k+S_i+\sigma^2 I)).$$

```{r}
U.ed.diag = lapply(ed.out$Ulist,function(z){z+(0.113)^2*diag(5)})
m.ed.diag = mash(data, U.ed.diag,verbose = F)
m.c.ed.diag = mash(data, c(U.c,U.ed.diag),verbose = F)
result$m.ed.diag = m.ed.diag
result$m.c.ed.diag = m.c.ed.diag
out = summary_out(simdata$B,result)
knitr::kable(out$find_cond,caption = 'On finding conditions',digits = 5)
```

```{r}
hist(m.c.ed.diag$result$lfsr,breaks = 100,xlab='lfsr',main="Histogram of lfsr")
hist(m.c.ed.diag$result$lfsr[which(simdata$B==0)],breaks = 100,xlab='lfsr',main="Histogram of lfsr for those beta=0")
plot(c(simdata$B), c(m.c.ed.diag$result$lfsr),col=1,pch=".",xlab='beta',ylab='lfsr')
abline(h = 0.05,lty=2,col='grey80')
abline(h = 0.1,lty=2,col='grey80')
```

How about a smaller $\sigma^2$? say $\sigma^2=0.01^2$

```{r}
U.ed.diag = lapply(ed.out$Ulist,function(z){z+(0.01)^2*diag(5)})
m.ed.diag = mash(data, U.ed.diag,verbose = F)
m.c.ed.diag = mash(data, c(U.c,U.ed.diag),verbose = F)
result$m.ed.diag = m.ed.diag
result$m.c.ed.diag = m.c.ed.diag
out = summary_out(simdata$B,result)
knitr::kable(out$find_cond,caption = 'On finding conditions',digits = 5)
```

```{r}
#hist(m.c.ed.diag$result$lfsr,breaks = 100,xlab='lfsr',main="Histogram of lfsr")
hist(m.c.ed.diag$result$lfsr[which(simdata$B==0)],breaks = 100,xlab='lfsr',main="Histogram of lfsr for those beta=0")
#plot(c(simdata$B), c(m.c.ed.diag$result$lfsr),col=1,pch=".",xlab='beta',ylab='lfsr')
#abline(h = 0.05,lty=2,col='grey80')
#abline(h = 0.1,lty=2,col='grey80')
```

Further reduce $\sigma^2$ to $0.005^2$.

```{r}
U.ed.diag = lapply(ed.out$Ulist,function(z){z+(0.005)^2*diag(5)})
m.ed.diag = mash(data, U.ed.diag,verbose = F)
m.c.ed.diag = mash(data, c(U.c,U.ed.diag),verbose = F)
result$m.ed.diag = m.ed.diag
result$m.c.ed.diag = m.c.ed.diag
out = summary_out(simdata$B,result)
knitr::kable(out$find_cond,caption = 'On finding conditions',digits = 5)
```

(How to decide the size of $\sigma^2$)

## One more example

Now we generate data from a mixture of 4 multivariate normal distributions. In addition to the two above, we add point-mass and identity ones. 

```{r}
simple_sims2 = function (nsamp = 100, err_sd = 0.01){
    ncond = 5
    b1 = rnorm(nsamp)
    B.1 = matrix(cbind(b1, b1, 0, 0, 0), nrow = nsamp, ncol = ncond)
    b2 = rnorm(nsamp)
    B.2 = matrix(cbind(0, 0, b2, b2, b2), nrow = nsamp, ncol = ncond)
    
    B.id = matrix(rnorm(nsamp * ncond), nrow = nsamp, ncol = ncond)
    B.zero = matrix(0, nrow = nsamp, ncol = ncond)
    
    B = rbind(B.zero,B.id,B.1, B.2)
    Shat = matrix(err_sd, nrow = nrow(B), ncol = ncol(B))
    E = matrix(rnorm(length(Shat), mean = 0, sd = Shat), nrow = nrow(B), 
        ncol = ncol(B))
    Bhat = B + E
    row_ids = paste0("effect_", 1:nrow(B))
    col_ids = paste0("condition_", 1:ncol(B))
    rownames(B) = row_ids
    colnames(B) = col_ids
    rownames(Bhat) = row_ids
    colnames(Bhat) = col_ids
    rownames(Shat) = row_ids
    colnames(Shat) = col_ids
    U = matrix(0,nrow=ncond,ncol=ncond)
    U2 = U
    U2[1:2,1:2] = 1
    U3 = U
    U3[3:5,3:5] = 1
    U.true = list(U1 = matrix(0,nrow=ncond,ncol=ncond),
                  U2=U2,
                  U3=U3,
                  U4 = diag(ncond))
    return(list(B = B, Bhat = Bhat, Shat = Shat,U.true=U.true))
}

set.seed(12345)
simdata = simple_sims2(500,err_sd = 0.1)

data = mash_set_data(simdata$Bhat,simdata$Shat)
m.1by1 = mash_1by1(data)
strong = get_significant_results(m.1by1)
#strong = 1:nrow(simdata$B)
U.c    = cov_canonical(data)
U.pca  = cov_pca(data,5,strong)
ed.out = bovy_wrapper(data,U.pca,strong)

```

Estimated mixing proportions: 

```{r}
round(ed.out$pi,2)
```

Estimated covariance matrices:

```{r}
lapply(ed.out$Ulist,round,digits=3)
```

```{r}
result = simu_study(simdata,strong)
```

```{r}
l = 100
sigma_seq = seq(0,1,length.out = l)
llik = c()
for(i in 1:l){
  #print(i)
  llik[i] = calc_loglikx(data,strong,ed.out$pi,ed.out$Ulist,sigma_seq[i])
}
plot(sigma_seq,llik,type='l',xlab = 'sigma',ylab='log likelihood')
abline(v = sigma_seq[which.max(llik)],lty=2)
text(x = sigma_seq[which.max(llik)]+0.03,y = min(llik), labels=paste('sigma=',round(sigma_seq[which.max(llik)],3),sep = ''))
```

We first set $\sigma^2 = 0.091^2$, as it's the MLE.

```{r}
U.ed.diag = lapply(ed.out$Ulist,function(z){z+(0.09)^2*diag(5)})
m.ed.diag = mash(data, U.ed.diag,verbose = F)
m.c.ed.diag = mash(data, c(U.c,U.ed.diag),verbose = F)
result$m.ed.diag = m.ed.diag
result$m.c.ed.diag = m.c.ed.diag
out = summary_out(simdata$B,result)
knitr::kable(out$find_cond,caption = 'On finding conditions',digits = 5)
```

```{r}
hist(m.c.ed.diag$result$lfsr,breaks = 100,xlab='lfsr',main="Histogram of lfsr")
hist(m.c.ed.diag$result$lfsr[which(simdata$B==0)],breaks = 100,xlab='lfsr',main="Histogram of lfsr for those beta=0")
#plot(c(simdata$B), c(m.c.ed.diag$result$lfsr),col=1,pch=".",xlab='beta',ylab='lfsr')
#abline(h = 0.05,lty=2,col='grey80')
#abline(h = 0.1,lty=2,col='grey80')
```

Then we set $\sigma^2$ such that the log-likelihood drops by a factor of 2(compare to the maximum), where $\sigma^2 = 1.2^2$.

```{r}
U.ed.diag = lapply(ed.out$Ulist,function(z){z+(1.2)^2*diag(5)})
m.ed.diag = mash(data, U.ed.diag,verbose = F)
m.c.ed.diag = mash(data, c(U.c,U.ed.diag),verbose = F)
result$m.ed.diag = m.ed.diag
result$m.c.ed.diag = m.c.ed.diag
out = summary_out(simdata$B,result)
knitr::kable(out$find_cond,caption = 'On finding conditions',digits = 5)
```

```{r}
hist(m.c.ed.diag$result$lfsr,breaks = 100,xlab='lfsr',main="Histogram of lfsr")
hist(m.c.ed.diag$result$lfsr[which(simdata$B==0)],breaks = 100,xlab='lfsr',main="Histogram of lfsr for those beta=0")
#plot(c(simdata$B), c(m.c.ed.diag$result$lfsr),col=1,pch=".",xlab='beta',ylab='lfsr')
#abline(h = 0.05,lty=2,col='grey80')
#abline(h = 0.1,lty=2,col='grey80')
```

