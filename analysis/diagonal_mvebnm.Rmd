---
title: "Add diagonal term to ED and check log likelihood"
author: "DongyueXie"
date: "2020-09-10"
output:
  workflowr::wflow_html:
    code_folding: hide
editor_options:
  chunk_output_type: console
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = F,message = F)
```

## Introduction

In my previous [example](check_mash.html), I found that using data driven covariance matrices from ED in prior results in high fdp, mainly due to small lfsr/lfdr. One reason is that there's uncertainty in estimated covaraince matrices but we did not take it into consideration. Matthew suggested adding a diagonal term into model, see if it could deal with the issue. 

The model is $$x_i\sim \sum_k\pi_k N(0,U_k+S_i),$$ where $S_i$ is known, and the simplest way to add diagonal term is to add the same one to every mixture component, $$x_i\sim \sum_k\pi_kN(0,U_k+S_i+\sigma^2 I).$$

The $\sigma^2$ can be estimated by MLE but we suspect it will be estimated as 0 often. Before we derive the model, we first do a quick check suggested by Matthew. We first obtain estimates of $\pi_k,U_k$ from ED(without diagonal term) and fix them, then add $\sigma^2I$, calculate log likelihood, and plot log-lik vs $\sigma^2$. The log likelihood is $$\log L(X) = \sum_i\log(\sum_k\pi_k N(x_i;0,U_k+S_i+\sigma^2 I)).$$



```{r}
library(mashr)
library(mvtnorm)

n_conditions = function(data){ncol(data$Bhat)}

n_effects = function(data){nrow(data$Bhat)}

bovy_wrapper = function(data, Ulist_init, subset=NULL, ...){
  if(is.null(subset)){subset = 1:n_effects(data)}
  K = length(Ulist_init)
  R = n_conditions(data)
  pi_init = rep(1/K, K) # initial mix proportions
  D = ncol(data$V)
  if(all(data$V==diag(D))){
    ed.res = extreme_deconvolution(data$Bhat[subset,],
                                   data$Shat[subset,]^2,
                                   xamp = pi_init,
                                   xmean = matrix(0,nrow=K,ncol=R),
                                   xcovar = Ulist_init,
                                   fixmean = TRUE,
                                   ...)
  }else{
    if(!is.null(data$L)){
      ycovar = lapply(subset, function(i) data$L %*% (data$Shat_orig[i,] * t(data$V * data$Shat_orig[i,])) %*% t(data$L) )
    }else{
      ycovar = lapply(subset, function(i) data$Shat[i,] * t(data$V * data$Shat[i,]) )
    }
    ed.res = extreme_deconvolution(data$Bhat[subset,],
                                   ycovar,
                                   xamp = pi_init,
                                   xmean = matrix(0,nrow=K,ncol=R),
                                   xcovar = Ulist_init,
                                   fixmean = TRUE,
                                   ...)
  }
  return(list(pi = ed.res$xamp, Ulist = ed.res$xcovar, av_loglik = ed.res$avgloglikedata))
}

calc_loglikx = function(data,subset,pihat,Ulist,sigma){
  
  n = length(subset)
  loglik = 0
  for(i in subset){
    loglik = loglik + mixture_loglikx(data$Bhat[i,],data$Shat[i,],pihat,Ulist,sigma)
  }
  loglik
}

mixture_loglikx = function(x,shat,pihat,Ulist,sigma){
  
  K = length(pihat)
  p = length(x)
  lik = 0
  for(k in 1:K){
    #browser()
    lik = lik + pihat[k]*dmvnorm(x,sigma = Ulist[[k]]+diag(shat^2)+sigma^2*diag(p))
  }
  log(lik)
}
```

## Two mixture components

Now we generate data from a mixture of 2 MVN with the following covariance matrices: present (and identical) in first two conditions, present (and identical) in last three conditions, each with 500 samples(so $\pi_1=\pi_2=0.5$), and $s_i=0.01$. 

Two covariance matrices are:

```{r}
cov1 = c(1,1,0,0,0)%*%t(c(1,1,0,0,0))
cov2 = c(0,0,1,1,1)%*%t(c(0,0,1,1,1))
print(cov1)
print(cov2)
```

```{r}
simple_sims0 = function(nsamp = 100, err_sd = 0.01){
    ncond = 5
    b1 = rnorm(nsamp)
    B.1 = matrix(cbind(b1, b1, 0, 0, 0), nrow = nsamp, ncol = ncond)
    b2 = rnorm(nsamp)
    B.2 = matrix(cbind(0, 0, b2, b2, b2), nrow = nsamp, ncol = ncond)
    
    #B.id = matrix(rnorm(nsamp * ncond), nrow = nsamp, ncol = ncond)
    #B.zero = matrix(0, nrow = nsamp, ncol = ncond)
    
    B = rbind(B.1, B.2)
    Shat = matrix(err_sd, nrow = nrow(B), ncol = ncol(B))
    E = matrix(rnorm(length(Shat), mean = 0, sd = Shat), nrow = nrow(B), ncol = ncol(B))
    Bhat = B + E
    row_ids = paste0("effect_", 1:nrow(B))
    col_ids = paste0("condition_", 1:ncol(B))
    rownames(B) = row_ids
    colnames(B) = col_ids
    rownames(Bhat) = row_ids
    colnames(Bhat) = col_ids
    rownames(Shat) = row_ids
    colnames(Shat) = col_ids
    U = matrix(0,nrow=ncond,ncol=ncond)
    U2 = U
    U2[1:2,1:2] = 1
    U3 = U
    U3[3:5,3:5] = 1
    U.true = list(#U1 = matrix(0,nrow=ncond,ncol=ncond),
                  U2=U2,
                  U3=U3)
                  #U4 = diag(ncond))
    return(list(B = B, Bhat = Bhat, Shat = Shat,U.true=U.true))
}

set.seed(12345)
simdata = simple_sims0(500,err_sd = 0.01)
data = mash_set_data(simdata$Bhat,simdata$Shat)
#m.1by1 = mash_1by1(data)
#strong = get_significant_results(m.1by1)
strong = 1:nrow(simdata$B)
U.c    = cov_canonical(data)
U.pca  = cov_pca(data,5,strong)
ed.out = bovy_wrapper(data,U.pca,strong)
```

Let's look at ED estimates.

Estimated mixing proportions: 

```{r}
round(ed.out$pi,2)
```

Estimated covariance matrices:

```{r}
lapply(ed.out$Ulist,round,digits=3)
```

```{r}
l = 100
sigma_seq = seq(0,0.4,length.out = l)
llik = c()
for(i in 1:l){
  #print(i)
  llik[i] = calc_loglikx(data,strong,ed.out$pi,ed.out$Ulist,sigma_seq[i])
}
plot(sigma_seq,llik,type='l',xlab = 'sigma',ylab='log likelihood')
abline(v = sigma_seq[which.max(llik)],lty=2)
text(x = sigma_seq[which.max(llik)]+0.03,y = min(llik), labels=paste('sigma=',round(sigma_seq[which.max(llik)],3),sep = ''))
```

```{r}
sigma_seq[which.min(abs(llik[1]/2-llik))]
```

Log likelihood peaks at $\sigma=0$. In this example, $l(\sigma=0;X)\approx 2\times l(\sigma=0.0566;X)$.

Now let's increase the standard error $s_i$ to $0.1$.

```{r}
simdata = simple_sims0(500,err_sd = 0.1)
data = mash_set_data(simdata$Bhat,simdata$Shat)
#m.1by1 = mash_1by1(data)
#strong = get_significant_results(m.1by1)
strong = 1:nrow(simdata$B)
U.c    = cov_canonical(data)
U.pca  = cov_pca(data,5,strong)
ed.out = bovy_wrapper(data,U.pca,strong)
```


Estimated mixing proportions: 

```{r}
round(ed.out$pi,2)
```

Estimated covariance matrices:

```{r}
lapply(ed.out$Ulist,round,digits=3)
```

```{r}
l = 100
sigma_seq = seq(0,0.4,length.out = l)
llik = c()
for(i in 1:l){
  #print(i)
  llik[i] = calc_loglikx(data,strong,ed.out$pi,ed.out$Ulist,sigma_seq[i])
}
plot(sigma_seq,llik,type='l',xlab = 'sigma',ylab='log likelihood')
abline(v = sigma_seq[which.max(llik)],lty=2)
text(x = sigma_seq[which.max(llik)]+0.022,y = min(llik), labels=paste('sigma=',round(sigma_seq[which.max(llik)],3),sep = ''))
```

```{r}
sigma_seq[which.min(abs(llik[1]/2-llik))]
```

Log likelihood peaks at $\sigma=0$. In this example, $l(\sigma=0;X)\approx 2\times l(\sigma=0.113;X)$.

## Three mixture components

Let's add one more covariance matrix, a rank-2 matrix.

```{r}
cov3 = c(1,1,1,0,0)%*%t(c(1,1,1,0,0))
cov4 = c(1,1,1,1,0)%*%t(c(1,1,1,1,0))
print(cov3+cov4)
```

```{r}
simple_sims3 = function(nsamp = 100, err_sd = 0.01){
    ncond = 5
    b1 = rnorm(nsamp)
    B.1 = matrix(cbind(b1, b1, 0, 0, 0), nrow = nsamp, ncol = ncond)
    b2 = rnorm(nsamp)
    B.2 = matrix(cbind(0, 0, b2, b2, b2), nrow = nsamp, ncol = ncond)
    
    U4 = c(1,1,1,0,0)%*%t(c(1,1,1,0,0))+c(1,1,1,1,0)%*%t(c(1,1,1,1,0))
    B.3 = rmvnorm(nsamp,sigma = U4)
    
    #B.id = matrix(rnorm(nsamp * ncond), nrow = nsamp, ncol = ncond)
    #B.zero = matrix(0, nrow = nsamp, ncol = ncond)
    
    B = rbind(B.1, B.2,B.3)
    Shat = matrix(err_sd, nrow = nrow(B), ncol = ncol(B))
    E = matrix(rnorm(length(Shat), mean = 0, sd = Shat), nrow = nrow(B), ncol = ncol(B))
    Bhat = B + E
    row_ids = paste0("effect_", 1:nrow(B))
    col_ids = paste0("condition_", 1:ncol(B))
    rownames(B) = row_ids
    colnames(B) = col_ids
    rownames(Bhat) = row_ids
    colnames(Bhat) = col_ids
    rownames(Shat) = row_ids
    colnames(Shat) = col_ids
    U = matrix(0,nrow=ncond,ncol=ncond)
    U2 = U
    U2[1:2,1:2] = 1
    U3 = U
    U3[3:5,3:5] = 1
    U4 = 
    U.true = list(#U1 = matrix(0,nrow=ncond,ncol=ncond),
                  U2=U2,
                  U3=U3,
                  U4 = U4)
    return(list(B = B, Bhat = Bhat, Shat = Shat,U.true=U.true))
}

set.seed(12345)
simdata = simple_sims3(500,err_sd = 0.01)
data = mash_set_data(simdata$Bhat,simdata$Shat)
#m.1by1 = mash_1by1(data)
#strong = get_significant_results(m.1by1)
strong = 1:nrow(simdata$B)
U.c    = cov_canonical(data)
U.pca  = cov_pca(data,5,strong)
ed.out = bovy_wrapper(data,U.pca,strong)
```

Let's look at ED estimates.

Estimated mixing proportions: 

```{r}
round(ed.out$pi,2)
```

Estimated covariance matrices:

```{r}
lapply(ed.out$Ulist,round,digits=3)
```

```{r}
l = 100
sigma_seq = seq(0,0.4,length.out = l)
llik = c()
for(i in 1:l){
  #print(i)
  llik[i] = calc_loglikx(data,strong,ed.out$pi,ed.out$Ulist,sigma_seq[i])
}
plot(sigma_seq,llik,type='l',xlab = 'sigma',ylab='log likelihood')
abline(v = sigma_seq[which.max(llik)],lty=2)
text(x = sigma_seq[which.max(llik)]+0.03,y = min(llik), labels=paste('sigma=',round(sigma_seq[which.max(llik)],3),sep = ''))
```

Now let's increase the standard error $s_i$ to $0.5$.

```{r}

simdata = simple_sims3(500,err_sd = 0.5)
data = mash_set_data(simdata$Bhat,simdata$Shat)
#m.1by1 = mash_1by1(data)
#strong = get_significant_results(m.1by1)
strong = 1:nrow(simdata$B)
U.c    = cov_canonical(data)
U.pca  = cov_pca(data,5,strong)
ed.out = bovy_wrapper(data,U.pca,strong)
```

Let's look at ED estimates.

Estimated mixing proportions: 

```{r}
round(ed.out$pi,2)
```

Estimated covariance matrices:

```{r}
lapply(ed.out$Ulist,round,digits=3)
```

```{r}
l = 100
sigma_seq = seq(0,0.4,length.out = l)
llik = c()
for(i in 1:l){
  #print(i)
  llik[i] = calc_loglikx(data,strong,ed.out$pi,ed.out$Ulist,sigma_seq[i])
}
plot(sigma_seq,llik,type='l',xlab = 'sigma',ylab='log likelihood')
abline(v = sigma_seq[which.max(llik)],lty=2)
text(x = sigma_seq[which.max(llik)]+0.03,y = min(llik), labels=paste('sigma=',round(sigma_seq[which.max(llik)],3),sep = ''))
```

## void

Now we generate data from a mixture of 3 MVN with the following covariance matrices: identity, present (and identical) in first two conditions, present (and identical) in last three conditions, each with 500 samples(so $\pi_1=\pi_2=\pi_3=1/3$), and $s_i=0.01$.

```{r}
simple_sims3 = function(nsamp = 100, err_sd = 0.01){
    ncond = 5
    b1 = rnorm(nsamp)
    B.1 = matrix(cbind(b1, b1, 0, 0, 0), nrow = nsamp, ncol = ncond)
    b2 = rnorm(nsamp)
    B.2 = matrix(cbind(0, 0, b2, b2, b2), nrow = nsamp, ncol = ncond)
    
    B.id = matrix(rnorm(nsamp * ncond), nrow = nsamp, ncol = ncond)
    #B.zero = matrix(0, nrow = nsamp, ncol = ncond)
    
    B = rbind(B.id,B.1, B.2)
    Shat = matrix(err_sd, nrow = nrow(B), ncol = ncol(B))
    E = matrix(rnorm(length(Shat), mean = 0, sd = Shat), nrow = nrow(B), ncol = ncol(B))
    Bhat = B + E
    row_ids = paste0("effect_", 1:nrow(B))
    col_ids = paste0("condition_", 1:ncol(B))
    rownames(B) = row_ids
    colnames(B) = col_ids
    rownames(Bhat) = row_ids
    colnames(Bhat) = col_ids
    rownames(Shat) = row_ids
    colnames(Shat) = col_ids
    U = matrix(0,nrow=ncond,ncol=ncond)
    U2 = U
    U2[1:2,1:2] = 1
    U3 = U
    U3[3:5,3:5] = 1
    U.true = list(#U1 = matrix(0,nrow=ncond,ncol=ncond),
                  U2=U2,
                  U3=U3,
                  U4 = diag(ncond))
    return(list(B = B, Bhat = Bhat, Shat = Shat,U.true=U.true))
}
```

```{r}
set.seed(12345)
simdata = simple_sims3(50,err_sd = 0.01)
data = mash_set_data(simdata$Bhat,simdata$Shat)
#m.1by1 = mash_1by1(data)
#strong = get_significant_results(m.1by1)
strong = 1:nrow(simdata$B)
U.c    = cov_canonical(data)
U.pca  = cov_pca(data,5,strong)
ed.out = bovy_wrapper(data,U.pca,strong)
```


Estimated mixing proportions: 

```{r}
round(ed.out$pi,2)
```

Estimated covariance matrices:

```{r}
lapply(ed.out$Ulist,round,digits=3)
```


```{r}
l = 100
sigma_seq = seq(0,0.4,length.out = l)
llik = c()
for(i in 1:l){
  #print(i)
  llik[i] = calc_loglikx(data,strong,ed.out$pi,ed.out$Ulist,sigma_seq[i])
}
plot(sigma_seq,llik,type='l',xlab = 'sigma',ylab='log likelihood')
abline(v = sigma_seq[which.max(llik)],lty=2)
text(x = sigma_seq[which.max(llik)]+0.03,y = min(llik), labels=paste('sigma=',round(sigma_seq[which.max(llik)],3),sep = ''))
```


Now increase the standard error $s_i$ to $0.1$.

```{r}
simdata = simple_sims3(50,err_sd = 0.1)
data = mash_set_data(simdata$Bhat,simdata$Shat)
#m.1by1 = mash_1by1(data)
#strong = get_significant_results(m.1by1)
strong = 1:nrow(simdata$B)
U.c    = cov_canonical(data)
U.pca  = cov_pca(data,5,strong)
ed.out = bovy_wrapper(data,U.pca,strong)
```


Estimated mixing proportions: 

```{r}
round(ed.out$pi,2)
```

Estimated covariance matrices:

```{r}
lapply(ed.out$Ulist,round,digits=3)
```


```{r}
l = 100
sigma_seq = seq(0,0.4,length.out = l)
llik = c()
for(i in 1:l){
  #print(i)
  llik[i] = calc_loglikx(data,strong,ed.out$pi,ed.out$Ulist,sigma_seq[i])
}
plot(sigma_seq,llik,type='l',xlab = 'sigma',ylab='log likelihood')
abline(v = sigma_seq[which.max(llik)],lty=2)
text(x = sigma_seq[which.max(llik)]+0.03,y = min(llik), labels=paste('sigma=',round(sigma_seq[which.max(llik)],3),sep = ''))
```


Now likelihood peaks at $\sigma\neq 0$. The reason might be that ED does not output identity matrix, which is in the prior, and adding diagonal term accommodate this. Maybe this is not a very good example.


